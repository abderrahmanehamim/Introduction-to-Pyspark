{"cells":[{"source":"Creating a SparkSession","metadata":{},"id":"7607bdb2-1707-4361-a984-1c443fe84370","cell_type":"markdown"},{"source":"# Import SparkSession from pyspark.sql\nfrom pyspark.sql import SparkSession\n\n# Create my_spark\nmy_spark = SparkSession.builder.getOrCreate()\n\n# Print my_spark\nprint(my_spark)\n","metadata":{},"id":"7f26dd6a-20aa-4d19-8d3f-b8f64fba1821","cell_type":"code","execution_count":1,"outputs":[]},{"source":"Viewing tables : Once you've created a SparkSession, you can start poking around to see what data is in your cluster!\n\nYour SparkSession has an attribute called catalog which lists all the data inside the cluster. This attribute has a few methods for extracting different pieces of information.\n\nOne of the most useful is the .listTables() method, which returns the names of all the tables in your cluster as a list.","metadata":{},"cell_type":"markdown","id":"6c9f9176-66ee-47d9-8943-0beab09c5aa9"},{"source":"# Print the tables in the catalog\nprint(spark.catalog.listTables())","metadata":{},"cell_type":"code","id":"565b636f-13cf-4610-9633-d87d6599f80b","execution_count":null,"outputs":[]},{"source":"Use the .sql() method to get the first 10 rows of the flights table and save the result to flights10. The variable query contains the appropriate SQL query.\nUse the DataFrame method .show() to print flights10.","metadata":{},"cell_type":"markdown","id":"90d93ed9-a65e-4425-9a00-9ba7f836c0d1"},{"source":"# Don't change this query\nquery = \"FROM flights SELECT * LIMIT 10\"\n\n# Get the first 10 rows of flights\nflights10 = spark.sql(query)\n\n# Show the results\nflights10.show()","metadata":{},"cell_type":"code","id":"584d0f21-2836-4d4a-a7ba-d8fa6be880b9","execution_count":null,"outputs":[]},{"source":"#using panda on Spark DataFrame\n# Don't change this query\nquery = \"SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest\"\n\n# Run the query\nflight_counts = spark.sql(query)\n\n# Convert the results to a pandas DataFrame\npd_counts = flight_counts.toPandas()\n\n# Print the head of pd_counts\nprint(pd_counts.head())","metadata":{},"cell_type":"code","id":"ef78b6d6-2358-48c9-962c-1d71320be28a","execution_count":null,"outputs":[]},{"source":" put data into Spark via pandas","metadata":{},"cell_type":"markdown","id":"5cb93270-b20f-4ff6-8365-5b178ff0af4e"},{"source":"# Create pd_temp\npd_temp = pd.DataFrame(np.random.random(10))\n\n# Create spark_temp from pd_temp\nspark_temp = spark.createDataFrame(pd_temp)\n\n# Examine the tables in the catalog\nprint(spark.catalog.listTables())\n\n# Add spark_temp to the catalog\nspark_temp.createOrReplaceTempView(\"temp\")\n# Examine the tables in the catalog again\nprint(spark.catalog.listTables())","metadata":{},"cell_type":"code","id":"e8159c77-ed71-4c52-8052-16de247c1b9f","execution_count":null,"outputs":[]},{"source":"# Don't change this file path\nfile_path = \"/usr/local/share/datasets/airports.csv\"\n\n# Read in the airports data\nairports = spark.read.csv(file_path, header=True)\n\n# Show the data\nairports.show()","metadata":{},"cell_type":"code","id":"bf5167db-518b-4bf6-94f0-fd527cca6c22","execution_count":null,"outputs":[]},{"source":"Use the spark.table() method with the argument \"flights\" to create a DataFrame containing the values of the flights table in the .catalog. Save it as flights.\nShow the head of flights using flights.show(). Check the output: the column air_time contains the duration of the flight in minutes.\nUpdate flights to include a new column called duration_hrs, that contains the duration of each flight in hours (you'll need to divide air_time by the number of minutes in an hour).","metadata":{},"cell_type":"markdown","id":"fcedfe49-87fe-4dbc-a712-a6edb9da243f"},{"source":"# Create the DataFrame flights\nflights = spark.table(\"flights\")\n\n# Show the head\nflights.show()\n\n# Add duration_hrs\nflights = flights.withColumn(\"duration_hrs\",flights.air_time/60)","metadata":{},"cell_type":"code","id":"700fe9c7-eb46-4db9-87bb-6e739e9a6dc5","execution_count":null,"outputs":[]},{"source":"Use the .filter() method to find all the flights that flew over 1000 miles two ways:\nFirst, pass a SQL string to .filter() that checks whether the distance is greater than 1000. Save this as long_flights1.\nThen pass a column of boolean values to .filter() that checks the same thing. Save this as long_flights2.\nUse .show() to print heads of both DataFrames and make sure they're actually equal!","metadata":{},"cell_type":"markdown","id":"8a6a821d-bc02-44b2-95e0-0b563a411b9f"},{"source":"# Filter flights by passing a string\nlong_flights1 = flights.filter(\"distance > 1000\")\n\n# Filter flights by passing a column of boolean values\nlong_flights2 = flights.filter(flights.distance > 1000)\n\n# Print the data to check they're equal\nlong_flights1.show()\nlong_flights2.show()","metadata":{},"cell_type":"code","id":"44c02ae2-40eb-4235-9d46-e3331ea0a787","execution_count":null,"outputs":[]},{"source":"Select the columns \"tailnum\", \"origin\", and \"dest\" from flights by passing the column names as strings. Save this as selected1.\nSelect the columns \"origin\", \"dest\", and \"carrier\" using the df.colName syntax and then filter the result using both of the filters already defined for you (filterA and filterB) to only keep flights from SEA to PDX. Save this as selected2.","metadata":{},"cell_type":"markdown","id":"65a71ada-f80d-46ed-bcf1-c68cf1fd4393"},{"source":"# Select the first set of columns\nselected1 = flights.select(\"tailnum\", \"origin\", \"dest\")\n\n# Select the second set of columns\ntemp = flights.select(flights.origin, flights.dest, flights.carrier)\n\n# Define first filter\nfilterA = flights.origin == \"SEA\"\n\n# Define second filter\nfilterB = flights.dest == \"PDX\"\n\n# Filter the data, first by filterA then by filterB\nselected2 = temp.filter(filterA).filter(filterB)","metadata":{},"cell_type":"code","id":"d7bd7ad5-cc06-4198-832c-0bc321816b29","execution_count":null,"outputs":[]},{"source":"Create a table of the average speed of each flight both ways.\n\nCalculate average speed by dividing the distance by the air_time (converted to hours). Use the .alias() method name this column \"avg_speed\". Save the output as the variable avg_speed.\nSelect the columns \"origin\", \"dest\", \"tailnum\", and avg_speed (without quotes!). Save this as speed1.\nCreate the same table using .selectExpr() and a string containing a SQL expression. Save this as speed2.","metadata":{},"cell_type":"markdown","id":"55781977-5e69-4c39-96cb-9ce15474d17f"},{"source":"# Define avg_speed\navg_speed = (flights.distance/(flights.air_time/60)).alias(\"avg_speed\")\n\n# Select the correct columns\nspeed1 = flights.select(\"origin\", \"dest\", \"tailnum\", avg_speed)\n\n# Create the same table using a SQL expression\nspeed2 = flights.selectExpr(\"origin\", \"dest\", \"tailnum\", \"distance/(air_time/60) as avg_speed\")","metadata":{},"cell_type":"code","id":"39856d86-5a03-4840-9b43-8159a218609c","execution_count":null,"outputs":[]},{"source":"Find the length of the shortest (in terms of distance) flight that left PDX by first .filter()ing and using the .min() method. Perform the filtering by referencing the column directly, not passing a SQL string.\nFind the length of the longest (in terms of time) flight that left SEA by filter()ing and using the .max() method. Perform the filtering by referencing the column directly, not passing a SQL string.\n\n","metadata":{},"cell_type":"markdown","id":"4f0b4f72-feef-4841-b796-f732db61a225"},{"source":"# Find the shortest flight from PDX in terms of distance\nflights.filter(flights.origin == \"PDX\").groupBy().min(\"distance\").show()\n\n# Find the longest flight from SEA in terms of air time\nflights.filter(flights.origin == \"SEA\").groupBy().max(\"air_time\").show()","metadata":{},"cell_type":"code","id":"6729dbd6-a4f3-45ec-8395-3f602d9bb113","execution_count":null,"outputs":[]},{"source":" **I# AggregatingI**\n ","metadata":{},"cell_type":"markdown","id":"f48fd7e9-904f-442b-b67e-5ccfe8cabda0"},{"source":"To get you familiar with more of the built in aggregation methods, here's a few more exercises involving the flights table!\nUse the .avg() method to get the average air time of Delta Airlines flights (where the carrier column has the value \"DL\") that left SEA. The place of departure is stored in the column origin. show() the result.\nUse the .sum() method to get the total number of hours all planes in this dataset spent in the air by creating a column called duration_hrs from the column air_time. show() the result.","metadata":{},"cell_type":"markdown","id":"4271b0f9-35bf-4ae6-99e9-4e4c7d86fccf"},{"source":"# Average duration of Delta flights\nflights.filter(flights.origin == \"SEA\").filter(flights.carrier == \"DL\").groupBy().avg(\"air_time\").show()\n\n# Total hours in the air\nflights.withColumn(\"duration_hrs\", flights.air_time/60).groupBy().sum(\"duration_hrs\").show()","metadata":{},"cell_type":"code","id":"ee289908-ca44-4172-94f5-72d750356bdf","execution_count":null,"outputs":[]},{"source":"Create a DataFrame called by_plane that is grouped by the column tailnum.\nUse the .count() method with no arguments to count the number of flights each plane made.\nCreate a DataFrame called by_origin that is grouped by the column origin.\nFind the .avg() of the air_time column to find average duration of flights from PDX and SEA.","metadata":{},"cell_type":"markdown","id":"8120779a-f278-4704-a3d0-5654d2d0df6e"},{"source":"# Group by tailnum\nby_plane = flights.groupBy(\"tailnum\")\n\n# Number of flights each plane made\nby_plane.count().show()\n\n# Group by origin\nby_origin = flights.groupBy(\"origin\")\n\n# Average duration of flights from PDX and SEA\nby_origin.avg(\"air_time\").show()","metadata":{},"cell_type":"code","id":"11fedf00-fa28-49df-aec5-d2161b4eaf26","execution_count":null,"outputs":[]},{"source":"# **GROUPING AND AGGREGATION**\nImport the submodule pyspark.sql.functions as F.\nCreate a GroupedData table called by_month_dest that's grouped by both the month and dest columns. Refer to the two columns by passing both strings as separate arguments.\nUse the .avg() method on the by_month_dest DataFrame to get the average dep_delay in each month for each destination.\nFind the standard deviation of dep_delay by using the .agg() method with the function F.stddev().","metadata":{},"cell_type":"markdown","id":"a96d70e1-3692-4e9b-bc6f-01d9661b7fe7"},{"source":"# Import pyspark.sql.functions as F\nimport pyspark.sql.functions as F\n\n# Group by month and dest\nby_month_dest = flights.groupBy(\"month\",\"dest\")\n\n# Average departure delay by month and destination\nby_month_dest.avg(\"dep_delay\").show()\n\n# Standard deviation of departure delay\nby_month_dest.agg(F.stddev(\"dep_delay\")).show()","metadata":{},"cell_type":"code","id":"c01ae81a-7882-413d-bba0-4810978b2bdf","execution_count":null,"outputs":[]},{"source":"# **Joining II**\nExamine the airports DataFrame by calling .show(). Note which key column will let you join airports to the flights table.\nRename the faa column in airports to dest by re-assigning the result of airports.withColumnRenamed(\"faa\", \"dest\") to airports.\nJoin the flights with the airports DataFrame on the dest column by calling the .join() method on flights. Save the result as flights_with_airports.\nThe first argument should be the other DataFrame, airports.\nThe argument on should be the key column.\nThe argument how should be \"leftouter\".\nCall .show() on flights_with_airports to examine the data again. Note the new information that has been added.","metadata":{},"cell_type":"markdown","id":"3bc343b5-68bc-4f57-88db-ab84529085ab"},{"source":"# Examine the data\nprint(airports.show())\n\n# Rename the faa column\nairports = airports.withColumnRenamed(\"faa\",\"dest\")\n\n# Join the DataFrames\nflights_with_airports = flights.join(airports,on=\"dest\",how=\"leftouter\")\n\n# Examine the new DataFrame\nprint(flights_with_airports.show())","metadata":{},"cell_type":"code","id":"054c05d1-e153-499e-814a-bf7e02c1c30b","execution_count":null,"outputs":[]},{"source":"# **Getting started with machine learning pipelines**","metadata":{},"cell_type":"markdown","id":"14e3beff-d983-4b7c-b2b8-9e54795ce90d"},{"source":"1. First, rename the year column of planes to plane_year to avoid duplicate column names. \n\n2. Create a new DataFrame called model_data by joining the flights table with planes using the tailnum column as the key.","metadata":{},"cell_type":"markdown","id":"ad53e76e-da44-4b3b-bdd4-332ad8039ccd"},{"source":"# Rename year column\nplanes = planes.withColumnRenamed(\"year\",\"plane_year\")\n\n# Join the DataFrames\nmodel_data = flights.join(planes, on=\"tailnum\", how=\"leftouter\")","metadata":{},"cell_type":"code","id":"c367a031-b5d9-4a7e-93b4-8e438689fb03","execution_count":null,"outputs":[]},{"source":"String to integer\nNow you'll use the .cast() method you learned in the previous exercise to convert all the appropriate columns from your DataFrame model_data to integers!\n\nTo convert the type of a column using the .cast() method, you can write code like this:\n\ndataframe = dataframe.withColumn(\"col\", dataframe.col.cast(\"new_type\"))","metadata":{},"cell_type":"markdown","id":"0373a6ca-546b-4399-99e2-033d073eec39"},{"source":"# Cast the columns to integers\nmodel_data = model_data.withColumn(\"arr_delay\",model_data.arr_delay.cast(\"integer\"))\nmodel_data = model_data.withColumn(\"air_time\", model_data.air_time.cast(\"integer\"))\nmodel_data = model_data.withColumn(\"month\", model_data.month.cast(\"integer\"))\nmodel_data = model_data.withColumn(\"plane_year\", model_data.plane_year.cast(\"integer\"))","metadata":{},"cell_type":"code","id":"1156c762-53ba-4ce3-88a1-8432d920e4b6","execution_count":null,"outputs":[]},{"source":"Create the column plane_age using the .withColumn() method and subtracting the year of manufacture (column plane_year) from the year (column year) of the flight.","metadata":{},"cell_type":"markdown","id":"1b3b7fcb-af4f-4786-86aa-30709df78754"},{"source":"# Create the column plane_age\nmodel_data = model_data.withColumn(\"plane_age\", model_data.year - model_data.plane_year)","metadata":{},"cell_type":"code","id":"ce5f6d64-2a35-480c-a634-773fa948c852","execution_count":null,"outputs":[]},{"source":"# **Making a Boolean**\n# Consider that you're modeling a yes or no question: is the flight late? However, your data contains the arrival delay in minutes for each flight. Thus, you'll need to create a boolean column which indicates whether the flight was late or not!","metadata":{},"cell_type":"markdown","id":"58b56bf9-a590-489f-9393-6dde6ecf580f"},{"source":"Use the .withColumn() method to create the column is_late. This column is equal to model_data.arr_delay > 0.\nConvert this column to an integer column so that you can use it in your model and name it label (this is the default name for the response variable in Spark's machine learning routines).\nFilter out missing values (this has been done for you).","metadata":{},"cell_type":"markdown","id":"3878cd3a-e1e9-4ed5-b504-d99afdcb76d2"},{"source":"# Create is_late\nmodel_data = model_data.withColumn(\"is_late\", model_data.arr_delay > 0)\n\n# Convert to an integer\nmodel_data = model_data.withColumn(\"label\", model_data.is_late.cast(\"integer\"))\n\n# Remove missing values\nmodel_data = model_data.filter(\"arr_delay is not NULL and dep_delay is not NULL and air_time is not NULL and plane_year is not NULL\")","metadata":{},"cell_type":"code","id":"34aedc04-33c0-4a64-9259-a029f33c3d15","execution_count":null,"outputs":[]},{"source":"In this exercise you'll create a StringIndexer and a OneHotEncoder to code the carrier column. To do this, you'll call the class constructors with the arguments inputCol and outputCol.\n\nThe inputCol is the name of the column you want to index or encode, and the outputCol is the name of the new column that the Transformer should create.","metadata":{},"cell_type":"markdown","id":"fd4a6a0a-cae1-40b3-9153-c267a178af5d"},{"source":"**Create a StringIndexer called carr_indexer by calling StringIndexer() with inputCol=\"carrier\" and outputCol=\"carrier_index\".\nCreate a OneHotEncoder called carr_encoder by calling OneHotEncoder() with inputCol=\"carrier_index\" and outputCol=\"carrier_fact\".**","metadata":{},"cell_type":"markdown","id":"e5d5f6a9-588c-47a6-b784-996893c92c6b"},{"source":"# Create a StringIndexer\ncarr_indexer = StringIndexer(inputCol=\"carrier\", outputCol=\"carrier_index\")\n\n# Create a OneHotEncoder\ncarr_encoder = OneHotEncoder(inputCol=\"carrier_index\", outputCol=\"carrier_fact\")","metadata":{},"cell_type":"code","id":"9fb208cf-a759-4d6d-b5cf-0d4a98e0881d","execution_count":null,"outputs":[]},{"source":"# Create a StringIndexer\ndest_indexer = StringIndexer(inputCol=\"dest\",outputCol=\"dest_index\")\n\n# Create a OneHotEncoder\ndest_encoder = OneHotEncoder(inputCol=\"dest_index\",outputCol=\"dest_fact\")","metadata":{},"cell_type":"code","id":"aabab95f-c669-45a3-b6bb-aa9ddefcfeb6","execution_count":null,"outputs":[]},{"source":"# **Assemble a vector**\nThe last step in the Pipeline is to combine all of the columns containing our features into a single column. This has to be done before modeling can take place because every Spark modeling routine expects the data to be in this form. You can do this by storing each of the values from a column as an entry in a vector. Then, from the model's point of view, every observation is a vector that contains all of the information about it and a label that tells the modeler what value that observation corresponds to.\n\nBecause of this, the pyspark.ml.feature submodule contains a class called VectorAssembler. This Transformer takes all of the columns you specify and combines them into a new vector column.","metadata":{},"cell_type":"markdown","id":"802522fc-cdff-49c4-bbb6-52f6165799e4"},{"source":"# Make a VectorAssembler\nvec_assembler = VectorAssembler(inputCols=[\"month\", \"air_time\", \"carrier_fact\", \"dest_fact\", \"plane_age\"], outputCol=\"features\")","metadata":{},"cell_type":"code","id":"29618e9b-92d6-4182-8fcc-04bd6a120fd4","execution_count":null,"outputs":[]},{"source":"# **Create the pipeline**\nYou're finally ready to create a Pipeline!\n\nPipeline is a class in the pyspark.ml module that combines all the Estimators and Transformers that you've already created. This lets you reuse the same modeling process over and over again by wrapping it up in one simple object. Neat, right?","metadata":{},"cell_type":"markdown","id":"268e21a1-2fee-4956-af1b-26468a4a1eb5"},{"source":"# Import Pipeline\nfrom pyspark.ml import Pipeline\n\n# Make the pipeline\nflights_pipe = Pipeline(stages=[dest_indexer, dest_encoder, carr_indexer, carr_encoder, vec_assembler])","metadata":{},"cell_type":"code","id":"eb1a94aa-1d81-43db-abc6-cd7b4b80605f","execution_count":null,"outputs":[]},{"source":"# **Transform the data**\nHooray, now you're finally ready to pass your data through the Pipeline you created!","metadata":{},"cell_type":"markdown","id":"b4be5bf1-cb06-4388-8163-44cc59aa45b0"},{"source":"# Fit and transform the data\npiped_data = flights_pipe.fit(model_data).transform(model_data)","metadata":{},"cell_type":"code","id":"b612b6b0-fab4-442f-9269-f1eb2570dc96","execution_count":null,"outputs":[]},{"source":"# **SPLIT THE DATA**\nUse the DataFrame method .randomSplit() to split piped_data into two pieces, training with 60% of the data, and test with 40% of the data by passing the list [.6, .4] to the .randomSplit() method.","metadata":{},"cell_type":"markdown","id":"dae618e9-b766-4d5b-ad8d-6bc889fd6ee8"},{"source":"# Split the data into training and test sets\ntraining, test = piped_data.randomSplit([.6, .4])","metadata":{},"cell_type":"code","id":"87bc73d2-8e49-4ec4-a5f7-77c64a3e655e","execution_count":null,"outputs":[]},{"source":"# **Create the modeler**\nThe Estimator you'll be using is a LogisticRegression from the pyspark.ml.classification submodule.","metadata":{},"cell_type":"markdown","id":"de6f2515-9777-44ba-9e31-e39def9dc83c"},{"source":"# Import LogisticRegression\nfrom pyspark.ml.classification import LogisticRegression\n\n# Create a LogisticRegression Estimator\nlr = LogisticRegression()","metadata":{},"cell_type":"code","id":"e4e97ff6-b42b-47b9-9c54-ef8f6569f01f","execution_count":null,"outputs":[]},{"source":"To understand logistic regression, note the following key points:\n\n- **Basic Concept:**\n\nLogistic regression is a statistical model used for classification.\nUnlike linear regression, which predicts a numeric value, logistic regression predicts the probability of an event occurring, which falls between 0 and 1.\n\n- **Classification in Logistic Regression:**\n\nLogistic regression can be used for classification tasks.\nTo classify observations, you set a cutoff point (threshold) on the predicted probabilities.\nIf the predicted probability is above the threshold, the observation is classified as a positive outcome ('yes' or the event occurring).\nIf the predicted probability is below the threshold, the observation is classified as a negative outcome ('no' or the event not occurring).\n- **Hyperparameters:**\n\nIn logistic regression, there are hyperparameters, which are values not estimated from the data but supplied by the user.\nThese hyperparameters can be tuned to optimize the model's performance.\nTuning involves testing different values for hyperparameters to find the best settings for your specific task.","metadata":{},"cell_type":"markdown","id":"097576cb-5fc4-4992-b25f-74838ed19d0c"},{"source":"Here's the essential information about k-fold cross-validation:\n\n- **Purpose of Cross-Validation:**\n\nCross-validation is a method used to estimate a model's performance on unseen data, similar to the test dataset.\nIt helps assess how well a model generalizes to new, unseen data.\n- **K-Fold Cross-Validation:**\n\nIn k-fold cross-validation, the training data is divided into several partitions or \"folds.\"\nThe value of 'k' represents the number of partitions, and typically, you use a value like 3, 5, or 10.\nEach fold is used as a test set while the remaining folds are used for training.\nThe model is trained and evaluated 'k' times, with each fold serving as the test set once.\nThis process ensures that every data block is used for testing exactly once.\n- **Cross-Validation Error:**\n\nAfter each iteration (each fold used as a test set), the model's error is measured on the held-out partition.\nThese errors are averaged to calculate the cross-validation error, which is an estimate of how the model will perform on unseen data.\n- **Hyperparameter Tuning:**\n\nCross-validation is commonly used to select the best hyperparameters for a model.\nA grid of possible hyperparameter values is created, and the cross-validation error is used to compare different combinations.\nThe combination of hyperparameters that results in the lowest cross-validation error is typically chosen as the best model.\nIn your specific case, you are using k-fold cross-validation to tune hyperparameters, specifically elasticNetParam and regParam, for your logistic regression model. This process helps you select the best hyperparameter values for your model, improving its performance on unseen data.","metadata":{},"cell_type":"markdown","id":"3200ec80-b352-4cdb-8d10-4796a266adf4"},{"source":"# **Exactly! The cross validation error is an estimate of the model's error on the test set.**","metadata":{},"cell_type":"markdown","id":"45777f34-a08f-43be-b7a5-5288ec8ece17"},{"source":"# Create the evaluator\nThe first thing you need when doing cross validation for model selection is a way to compare different models. Luckily, the pyspark.ml.evaluation submodule has classes for evaluating different kinds of models. Your model is a binary classification model, so you'll be using the BinaryClassificationEvaluator from the pyspark.ml.evaluation module.\n\nThis evaluator calculates the area under the ROC. This is a metric that combines the two kinds of errors a binary classifier can make (false positives and false negatives) into a simple number. You'll learn more about this towards the end of the chapter!","metadata":{},"cell_type":"markdown","id":"48ff6c34-d0fe-4002-9cb1-e5d958e87e31"},{"source":"# Import the evaluation submodule\nimport pyspark.ml.evaluation as evals\n\n# Create a BinaryClassificationEvaluator\nevaluator = evals.BinaryClassificationEvaluator(metricName=\"areaUnderROC\")","metadata":{},"cell_type":"code","id":"1bfbd57d-8fc7-4eee-a0e9-128732ada18c","execution_count":null,"outputs":[]},{"source":"# **Make a grid**","metadata":{},"cell_type":"markdown","id":"8ea5ce68-93af-4dd4-a2b8-54c11ae610e1"},{"source":"Next, you need to create a grid of values to search over when looking for the optimal hyperparameters. The submodule pyspark.ml.tuning includes a class called ParamGridBuilder that does just that (maybe you're starting to notice a pattern here; PySpark has a submodule for just about everything!).\n\nYou'll need to use the .addGrid() and .build() methods to create a grid that you can use for cross validation. The .addGrid() method takes a model parameter (an attribute of the model Estimator, lr, that you created a few exercises ago) and a list of values that you want to try. The .build() method takes no arguments, it just returns the grid that you'll use later.","metadata":{},"cell_type":"markdown","id":"e0b4db01-7473-4c0b-b795-9c59975c9acd"},{"source":"## Import the submodule pyspark.ml.tuning under the alias tune.\n## Call the class constructor ParamGridBuilder() with no arguments. Save this as grid.\n## Call the .addGrid() method on grid with lr.regParam as the first argument and np.arange(0, .1, .01) as the second argument. This second call is a function from the numpy module (imported as np) that creates a list of numbers from 0 to .1, incrementing by .01. Overwrite grid with the result.\n## Update grid again by calling the .addGrid() method a second time create a grid for lr.elasticNetParam that includes only the values [0, 1].\n## Call the .build() method on grid and overwrite it with the output","metadata":{},"cell_type":"markdown","id":"eca72b0e-b9fc-4b88-a4bc-b55e9c0a9482"},{"source":"# Import the tuning submodule\nimport pyspark.ml.tuning as tune\n\n# Create the parameter grid\ngrid = tune.ParamGridBuilder()\n\n# Add the hyperparameter\ngrid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\ngrid = grid.addGrid(lr.elasticNetParam,[0, 1])\n\n# Build the grid\ngrid = grid.build()","metadata":{},"cell_type":"code","id":"d6a46161-956f-4c06-aa6c-e8a1d4a99994","execution_count":null,"outputs":[]},{"source":"# **Make the validator**\nThe submodule pyspark.ml.tuning also has a class called CrossValidator for performing cross validation. This Estimator takes the modeler you want to fit, the grid of hyperparameters you created, and the evaluator you want to use to compare your models.\n\nThe submodule pyspark.ml.tune has already been imported as tune. You'll create the CrossValidator by passing it the logistic regression Estimator lr, the parameter grid, and the evaluator you created in the previous exercises.","metadata":{},"cell_type":"markdown","id":"00aaed5e-f447-4c0f-a078-48f944c76388"},{"source":"1. Create a CrossValidator by calling tune.CrossValidator() with the arguments:\n-         estimator=lr\n-         estimatorParamMaps=grid\n-         evaluator=evaluator\n2. Name this object cv.","metadata":{},"cell_type":"markdown","id":"5f2bad38-bd6f-449f-b75d-5fe76dfa85f6"},{"source":"# Create the CrossValidator\ncv = tune.CrossValidator(estimator=lr,\n               estimatorParamMaps=grid,\n               evaluator=evaluator\n               )","metadata":{},"cell_type":"code","id":"874aae1b-3870-495f-a113-8b88f9cd4b2a","execution_count":null,"outputs":[]},{"source":"# **Fit the model(s)**\nYou're finally ready to fit the models and select the best one!\n\nUnfortunately, cross validation is a very computationally intensive procedure. Fitting all the models would take too long on DataCamp.\n\nTo do this locally you would use the code:\n\n    # Fit cross validation models\n    models = cv.fit(training)\n\n    # Extract the best model\n    best_lr = models.bestModel\nRemember, the training data is called training and you're using lr to fit a logistic regression model. Cross validation selected the parameter values regParam=0 and elasticNetParam=0 as being the best. These are the default values, so you don't need to do anything else with lr before fitting the model.","metadata":{},"cell_type":"markdown","id":"c29eb0eb-6029-4c52-a669-55a0b3e6ed9c"},{"source":"# Call lr.fit()\nbest_lr = lr.fit(training)\n\n# Print best_lr\nprint(best_lr)","metadata":{},"cell_type":"code","id":"1f6238b0-399f-4a19-99a6-2bed8e6f6b24","execution_count":null,"outputs":[]},{"source":"# **Evaluating binary classifiers**\nFor this course we'll be using a common metric for binary classification algorithms call the AUC, or area under the curve. In this case, the curve is the ROC, or receiver operating curve. The details of what these things actually measure isn't important for this course. All you need to know is that for our purposes, the closer the AUC is to one (1), the better the model is!\n\n\nIf you've created a perfect binary classification model, the AUC (Area Under the Curve) would be equal to 1.\n\nIn binary classification, the ROC curve (Receiver Operating Characteristic) is a graphical representation of the model's performance. The AUC measures the area under this ROC curve. A perfect model, which can perfectly distinguish between the two classes without any misclassifications, would have an AUC of 1, indicating perfect discriminatory power.","metadata":{},"cell_type":"markdown","id":"79e470e6-e9fe-4145-acad-30ea0da5425f"},{"source":"# **Evaluate the model**\nRemember the test data that you set aside waaaaaay back in chapter 3? It's finally time to test your model on it! You can use the same evaluator you made to fit the model.","metadata":{},"cell_type":"markdown","id":"f17ddcbd-8632-422e-8579-85f7a32e78cc"},{"source":"- Use your model to generate predictions by applying best_lr.transform() to the test data. Save this as test_results.\n- Call evaluator.evaluate() on test_results to compute the AUC. Print the output.","metadata":{},"cell_type":"markdown","id":"17d5a627-74e7-443d-a221-c6fe0fac992c"},{"source":"# Use the model to predict the test set\ntest_results = best_lr.transform(test)\n\n# Evaluate the predictions\nprint(evaluator.evaluate(test_results))","metadata":{},"cell_type":"code","id":"2eb628f4-4852-4150-8942-2f596777cd90","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}